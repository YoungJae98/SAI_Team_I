# 7주차 과제

# 문제 1: 이론

- 문제 1번

    ## 문제타입

    - [x]  이론
    - [ ]  실습

    ---

    ## 문제내용

    다음 문제 중 맞으면 O, 틀리면 X를 답해주세요.

    ```
    a. back propagation(chain rule)에서 f=wx+b, g=wx, f=g+b 일 때, w,x,b가 f에 미치는 영향을 파악하기 위해서는 적분값을 이용한다.
    b. 최소 3개의 네트워크만으로도 XOR이 Neural Net로 표현 가능하다는 것을 수학적으로 증명해냈다.
    c. 은닉층 안의 각 노드의 가중치를 w_i, 바이어스를 b_i라 할 때 각 노드가 다음 레이어의 노드로 보내는 값은 w_i*x + b_i이다.
    d. Forward propagation은 [Multinomial classification]과 유사한 형태를 가지고 있다.
    e. Back propagation에서 최적화의 계산 방향은 은닉층에서 시작해 앞으로 진행한다.
    ```

    ---

    ### 출제 정보

    **출제자**

    류현수, 문승재, 유명건, 하주현

    **검수자**

    검수자 1: 함유진

    검수자 2: 홍성현

    최종 검수자 : 전원

    **기타**

    출제팀: SAI 1, 10조

    출제일: 2021-06-23

    ### 출제의도

    : XOR의 Neural Net에 대해 이해하고 옳고 그름을 파악할 수 있다.

- 풀이 1번

    답: 
    a : X → back propagation(chain rule)에서 f=wx+b, g=wx, f=g+b 일 때, w,x,b가 f에 미치는 영향을 파악하기 위해서는 미분값을 이용한다.
    b : O
    c : O
    d : O
    e : X →  Back propagation에서 최적화의 계산 방향은 출력층에서 시작해 앞으로 진행한다

    ### 기여자

    모든 팀원이 검토하며 문제를 풀었다.

    ### 검수자

    이연경

    ### 참고한 자료

---

# 문제 2: 이론

- 문제 2번

    ### 문제타입

    - [x]  이론
    - [ ]  실습

    ---

    다음 문제 중 맞으면 O, 틀리면 X를 답하고 이를 알맞게 수정해주세요.

    ```
    a: Tensorboard는 어려운, 많은 학습들을 진행할 때, 이의 진행사항을 쉽게 확인할 수 있도록 도와준다.
    b: Tensorboard를 사용할 때, logging 할 tensor가 하나의 값을 가진다면, histogram을 사용한다.
    c: XOR조건을 만족시키는 조합은 한 개만 존재한다.
    d: Deeplearning에서 여러 개가 쌓인 레이어들은 이전 레이어의 Y출력값과 다음 w변수의 X입력값이 같음을 만족한다.
    e: Perceptron은 신호가 유효한지 아닌지를 판단해주는 알고리즘인데, 기준치 이상의 신호가 들어오면 1을 도출하고, 그렇지 않으면 0을 결과값으로 보여준다.
    ```

    ### 출제 정보

    **출제자**

    문예완, 이세희, 이지연, 고규환, 박성열

    **검수자**

    검수자 : 1팀 전원

    **기타**

    출제팀: SAI 1, 10조

    출제일: 2021-06-23

    ### 출제의도

    :  강의내용 확인!! 

- 풀이 2번

    답: 
    a : O
    b : X →  Tensorboard를 사용할 때, logging 할 tensor가 여러 개의 값을 가진다면, histogram을 사용한다.
    c : X → XOR조건을 만족시키는 조합은 여러 개 존재할 수 있다
    d : O
    e : O

    ---

    ### 기여자

    모든 팀원이 검토하며 문제를 풀었다.

    ### 검수자

    이연경

---

# 문제 3: 이론

- 문제 3번

    ### 문제타입

    - [x]  이론
    - [x]  실습

    ---

    ### 문제내용

    ![](https://github.com/YoungJae98/SAI_Team_I/blob/main/7%EC%A3%BC%EC%B0%A8/7%EC%A3%BC%EC%B0%A8%20%ED%8C%8C%EC%9D%BC/Untitled.jpg)

    ![](https://github.com/YoungJae98/SAI_Team_I/blob/main/7%EC%A3%BC%EC%B0%A8/7%EC%A3%BC%EC%B0%A8%20%ED%8C%8C%EC%9D%BC/Untitled01.jpg)
    
    ![](https://github.com/YoungJae98/SAI_Team_I/blob/main/7%EC%A3%BC%EC%B0%A8/7%EC%A3%BC%EC%B0%A8%20%ED%8C%8C%EC%9D%BC/Untitled%202.jpg)

    ### 출제 정보

    **출제자**

    하주현

    **검수자**

    검수자 1: 문승재

    검수자 2: 류현수

    최종 검수자: 전원

    **기타**

    출제팀: SAI 1, 10조

    출제일: 2021-06-23

    ### 출제의도

    Neural Net 를 이용하여 XOR 문제를 풀어볼 수 있다. 

- 풀이 3번

    ![](https://github.com/YoungJae98/SAI_Team_I/blob/main/7%EC%A3%BC%EC%B0%A8/7%EC%A3%BC%EC%B0%A8%20%ED%8C%8C%EC%9D%BC/Untitled%203.jpg)

    ![](https://github.com/YoungJae98/SAI_Team_I/blob/main/7%EC%A3%BC%EC%B0%A8/7%EC%A3%BC%EC%B0%A8%20%ED%8C%8C%EC%9D%BC/Untitled%204.jpg)

    ### 기여자

    모든 팀원이 검토하며 문제를 풀었다.

    앙가영 : 각 과정을 상세히 정리했다.

    ### 검수자

    이연경

---

# 문제4 실습

- 문제 4번

    ### 문제타입

    - [x]  이론
    - [x]  실습

    ---

    ### 문제내용

    ![](https://github.com/YoungJae98/SAI_Team_I/blob/main/7%EC%A3%BC%EC%B0%A8/7%EC%A3%BC%EC%B0%A8%20%ED%8C%8C%EC%9D%BC/Untitled_Diagram.jpg)

     위 그림을 이용해서 아래 값을 구해주세요.

    ![](https://github.com/YoungJae98/SAI_Team_I/blob/main/7%EC%A3%BC%EC%B0%A8/7%EC%A3%BC%EC%B0%A8%20%ED%8C%8C%EC%9D%BC/Untitled%205.jpg)

    ### 출제 정보

    **출제자**

    함유진

    **검수자**

    검수자 1: 문승재

    검수자 2: 홍성현

    최종 검수자: 전원

    **기타**

    출제팀: SAI 1, 10조

    출제일: 2021-06-23

    ### 출제의도

    Back Propagation을 손으로 계산해볼 수 있다.

- 풀이 4번

    ---

    ![](https://github.com/YoungJae98/SAI_Team_I/blob/main/7%EC%A3%BC%EC%B0%A8/7%EC%A3%BC%EC%B0%A8%20%ED%8C%8C%EC%9D%BC/Untitled%206.jpg)

    ![](https://github.com/YoungJae98/SAI_Team_I/blob/main/7%EC%A3%BC%EC%B0%A8/7%EC%A3%BC%EC%B0%A8%20%ED%8C%8C%EC%9D%BC/7.jpg)

    ### 기여자

    모든 팀원이 검토하며 문제를 풀었다.

    이연경 : 각 과정을 상세히 정리했다.

    ### 검수자

    이연경

---

# 문제 5: 실습

- 문제 5번

    ### 문제유형

    - [ ]  이론
    - [x]  실습

    ---

    ### 문제내용

    다음 문제에서는 iris데이터를 NN을 통하여 종을 맞추는 실습 문제입니다.

    ![](https://github.com/YoungJae98/SAI_Team_I/blob/main/7%EC%A3%BC%EC%B0%A8/7%EC%A3%BC%EC%B0%A8%20%ED%8C%8C%EC%9D%BC/Untitled%207.jpg)

    x 데이터는 위 그림과 같이 왼쪽부터 4가지 컬럼으로 이루어져 있습니다.

    y 데이터는 setosa, versicolor, virginica 의 3종류로 구분됩니다.

    ```python
    #import
    try:
      # This %tensorflow_version magic only works in Colab.
      %tensorflow_version 1.x
    except Exception:
      pass

    # For your non-Colab code, be sure you have tensorflow==1.15
    import tensorflow as tf
    assert tf.__version__.startswith('1')
    print(tf.__version__)

    import numpy as np
    import pandas as pd

    from sklearn.datasets import load_iris
    iris = load_iris()
    ```

    x데이터는 **[iris.data](http://iris.data)**,  y 데이터는 [**iris.target**](http://iris.target) 입니다.

    **아래 빈칸들을 채워서 실습을 해봅시다.**

    ```python
    x_data = #x 데이터 넣어주세요
    y_data = #y 데이터 넣고, 적절하게 바꿔주세요.

    X = tf.placeholder(tf.float32, [         ]) #shape을 주세요
    Y = tf.placeholder(tf.int32, [         ]) #shape을 주세요

    #one hot 처리 해주기
    nb_classes =    
    Y_one_hot = tf.one_hot(              )  
    Y_one_hot = tf.reshape(              )

    #Layer 
    #중간층 노드수는 자유롭게 설정해 주세요.
    #필요하다면 Layer를 늘려도 됩니다.
    with tf.name_scope("Layer1"):
        W1 = tf.Variable(tf.random_normal([        ]), name='weight1')
        b1 = tf.Variable(tf.random_normal([   ]), name='bias1')
        layer1 = tf.sigmoid(tf.matmul(     ) + b1)
        tf.summary.histogram("W1", W1)
        tf.summary.histogram("b1", b1)
        tf.summary.histogram("Layer1", layer1)

    with tf.name_scope("Layer2"):
        W2 = tf.Variable(tf.random_normal([              ]), name='weight2')
        b2 = tf.Variable(tf.random_normal([        ]), name='bias2')
        logits = tf.matmul(     ) + b2
        hypothesis = tf.              
        tf.summary.histogram("W2", W2)
        tf.summary.histogram("b2", b2)
        tf.summary.histogram("Hypothesis", hypothesis)

    with tf.name_scope("Cost"):
        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,
                                                                        labels=tf.stop_gradient([Y_one_hot])))
        tf.summary.scalar("Cost", cost)
        
    prediction = tf.               #one hot처리한 데이터를 예측해 봅시다!
    correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

    with tf.name_scope("Train"):
        train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)

    with tf.Session() as sess:
        merged_summary = tf.summary.merge_all()
        writer = tf.summary.FileWriter("./logs/xor_logs_r0_01")
        writer.add_graph(sess.graph)
        sess.run(tf.global_variables_initializer())

        for step in range(2001):
            _, summary, cost_val = sess.run(
                [train, merged_summary, cost], feed_dict={X: x_data, Y: y_data}
            )
            writer.add_summary(summary, global_step=step)
                                            
            if step % 100 == 0:
                print("Step: {:5}\tCost: {:.3f}".format(step, cost_val))

        # Let's see if *we can predict
        pred = sess.run(prediction,* feed_dict={X: x_data})
        
        for p, y in zip(pred, y_data.flatten()): 
            print("[{}] Prediction: {} True Y: {}".format(p == int(y), p, int(y)))
        
        h, p, a = sess.run(
            [hypothesis, prediction, accuracy], feed_dict={X: x_data, Y: y_data}
        )
        
        print(f"\ntarget:\n{iris.target} \nprediction:\n{p} \nAccuracy:\n{a}")
    ```

    ![](https://github.com/YoungJae98/SAI_Team_I/blob/main/7%EC%A3%BC%EC%B0%A8/7%EC%A3%BC%EC%B0%A8%20%ED%8C%8C%EC%9D%BC/Untitled%208.jpg)
    
    ![](https://github.com/YoungJae98/SAI_Team_I/blob/main/7%EC%A3%BC%EC%B0%A8/7%EC%A3%BC%EC%B0%A8%20%ED%8C%8C%EC%9D%BC/Untitled%209.jpg)

    위처럼 Accuracy가 **0.98** 이상 나오면 성공!

    ### tensorboard 보기!

    아래 코드를 추가하여 colab에서 tensorboard를 확인해 봅시다.

    ```python
    #tensorboard 
    %load_ext tensorboard
    %tensorboard --logdir logs
    ```

    ### 출제 정보

    **출제자**

    홍성현

    **검수자**

    검수자 1: 함유진

    검수자 2: 류현수

    최종 검수자: 전원

    **기타**

    출제팀: SAI 1, 10조

    출제일: 2021-06-23

    ### 출제의도

    9강의에서 배운 NN을 적용해보기!

- 풀이 5번

    ```python
    x_data = np.array(iris.data, dtype=np.float32)
    y_data = np.array(iris.target, dtype=np.int32).reshape(-1, 1)

    X = tf.placeholder(tf.float32, [None, 4]) #shape을 주세요
    Y = tf.placeholder(tf.int32, [None, 1]) #shape을 주세요

    #one hot 처리 해주기
    nb_classes = 3
    Y_one_hot = tf.one_hot(Y, nb_classes)
    Y_one_hot = tf.reshape(Y_one_hot,[-1,nb_classes])

    #Layer 
    #중간층 노드수는 자유롭게 설정해 주세요.
    #필요하다면 Layer를 늘려도 됩니다.
    with tf.name_scope("Layer1"):
        W1 = tf.Variable(tf.random_normal([4,4]), name='weight1')
        b1 = tf.Variable(tf.random_normal([4]), name='bias1')
        layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)
        tf.summary.histogram("W1", W1)
        tf.summary.histogram("b1", b1)
        tf.summary.histogram("Layer1", layer1)

    with tf.name_scope("Layer2"):
        W2 = tf.Variable(tf.random_normal([4,nb_classes]), name='weight2')
        b2 = tf.Variable(tf.random_normal([nb_classes]), name='bias2')
        logits = tf.matmul(layer1,W2) + b2
        hypothesis = tf.sigmoid(logits)          
        tf.summary.histogram("W2", W2)
        tf.summary.histogram("b2", b2)
        tf.summary.histogram("Hypothesis", hypothesis)

    with tf.name_scope("Cost"):
        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,
                                                                        labels=tf.stop_gradient([Y_one_hot])))
        tf.summary.scalar("Cost", cost)
        
    prediction = tf.argmax(hypothesis,1)  #one hot처리한 데이터를 예측해 봅시다!
    correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

    with tf.name_scope("Train"):
        train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)

    with tf.Session() as sess:
        merged_summary = tf.summary.merge_all()
        writer = tf.summary.FileWriter("./logs/xor_logs_r0_01")
        writer.add_graph(sess.graph)
        sess.run(tf.global_variables_initializer())

        for step in range(2001):
            _, summary, cost_val = sess.run(
                [train, merged_summary, cost], feed_dict={X: x_data, Y: y_data}
            )
            writer.add_summary(summary, global_step=step)
                                            
            if step % 100 == 0:
                print("Step: {:5}\tCost: {:.3f}".format(step, cost_val))

        # Let's see if we can predict
        pred = sess.run(prediction, feed_dict={X: x_data})
        
        for p, y in zip(pred, y_data.flatten()): 
            print("[{}] Prediction: {} True Y: {}".format(p == int(y), p, int(y)))
        
        h, p, a = sess.run(
            [hypothesis, prediction, accuracy], feed_dict={X: x_data, Y: y_data}
        )
        
        print(f"\ntarget:\n{iris.target} \nprediction:\n{p} \nAccuracy:\n{a}
    ```

    ![](https://github.com/YoungJae98/SAI_Team_I/blob/main/7%EC%A3%BC%EC%B0%A8/7%EC%A3%BC%EC%B0%A8%20%ED%8C%8C%EC%9D%BC/Untitled%2010.jpg)

    ---

    기여자

    모든 팀원이 검토하며 문제를 풀었다.

    ### 검수자

    이연경

---
