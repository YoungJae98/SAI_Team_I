# 네번째 과제

# 문제 1 : 이론

---

- 문제 1번

    ## 문제타입

    - [x]  이론
    - [ ]  실습

    ---

    ## 문제내용

    다음 Softmax Regression의 기본 원리의 설명 중 틀린 것을 모두 고르시오.

    ```
    a. Logistic regression의 기본 원리는 아주 작은 값과 아주 큰 값을 압축시키는 것이다.

    b. Binary classification만으로 Multinomial classification를 구현할 수 있다.

    c. Multinomial classification은 하나의 직선으로도 구현할 수 있다.

    d. Softmax는 Logistic classifier에서 도출된 Vector를 의미한다.
    ```

    **출제자**

    출제자 1 : 이영인

    **검수자**

    검수자1 : 4조 전원

    최종 검수자 : 신동준, 이명준, 전서현

    **기타**

    출제일자 **:** 2021.05.11

- 풀이 1번

    답: **(c) (d)**

    (a) logistic regression에서 사용되는 sigmoid 함수가 필요한 이유에 대한 설명이므로 맞다.

    (b) 맞다.

    (c) Multinomial classification은 독립변수 수만큼의 직선이 필요하다.

    (d) softmax는 logistic classifier에서 도출된 vector를 1. 0과 1의 사이의 확률이 되도록 2. 합했을 때 1이 되도록 바꾸어주는 것이다.

    ### 기여자

    강찬울, 김영재, 양가영, 이연경, 이현재: 스터디 시간에 다함께 의견 나누었다.

    ### 검수자

    이연경

# 문제 2. Softmax Classification 구현

---

- 문제 2번

    ## 문제타입

    - [ ]  이론
    - [x]  실습

    ---

    ## 문제내용

    다음 data와 Base code를 사용하여 softmax classfication을 구현하시오

    ```
    x_data = (a ~ e)
    y_data = (f ~ i)

    출력을 캡처하여 올려주세요.
    ```

    **Base Code**

    ```python

    import numpy as np
    import pandas as pd

    df = #파일 읽기
    x_data = #x_data 수정
    y_data = #y_data 수정

    X = tf.placeholder("float", #shape 수정)
    Y = tf.placeholder("float", #shape 수정)
    nb_classes =       

    W = tf.Variable(tf.random_normal(           ), name='weight')
    b = tf.Variable(tf.random_normal(           ), name='bias')

    #tf.nn.softmax computes softmax activations
    #softmax = exp(Logits) / reduce_sum(exp(Logits), dim)
    hypothesis =                              

    #Cross entropy cost/Loss
    cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

    #Launch graph
    with tf.Session() as sess:
    	sess.run(tf.global_variables_initializer())

    	for step in range(2001):
    		sess.run(optimizer, feed_dict={X: x_data, Y: y_data})
    		if step % 200 == 0:
    			print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))
    ```

    ---

    [data.csv](%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/data.csv)

    ---

    ## 출제 정보

    ### 출제의도

    softmax classification을 이해하였는지 확인하고 데이터 불러오기, 데이터 slicing을 적절히 확인한다.

    **출제자**

    출제자 1 : 전서현

    **검수자**

    검수자1 : 4조 전원

    최종 검수자 : 신동준, 이명준, 이영인

    **기타**

    출제일자 **:** 2021.05.11

- 풀이 2번

    ```python
    import numpy as np
    import pandas as pd

    df = pd.read_csv('data.csv') #파일 읽기
    x_data = df.iloc[:,0:5] #x_data 수정
    y_data = df.iloc[:,5:9] #y_data 수정

    X = tf.placeholder("float", [None,5]) #shape 수정
    Y = tf.placeholder("float", [None,4]) #shape 수정
    nb_classes = 4

    W = tf.Variable(tf.random_normal([5,nb_classes]), name='weight')
    b = tf.Variable(tf.random_normal([nb_classes]), name='bias')

    #tf.nn.softmax computes softmax activations
    #softmax = exp(Logits) / reduce_sum(exp(Logits), dim)
    hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)                       

    #Cross entropy cost/Loss
    cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

    #Launch graph
    with tf.Session() as sess:
    	sess.run(tf.global_variables_initializer())

    	for step in range(2001):
    		sess.run(optimizer, feed_dict={X: x_data, Y: y_data})
    		if step % 200 == 0:
    			print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))
    ```

    ---

    **<출력 결과>**

    ![%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/4.png](%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/4.png)

    ### 기여자

    강찬울, 김영재, 양가영, 이연경, 이현재: 스터디 시간에 코드를 비교해보니 모두 동일하게 나왔음을 확인할 수 있었다.

    ### 검수자

    이연경

# 문제 3 : 이론

- 문제 3번

    ## 문제 타입

    - [x]  이론
    - [ ]  실습

    ---

    ### 문제내용

    ![%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/Untitled.png](%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/Untitled.png)

    ```
    (6-2강)
    Logistic cost 함수와 Cross entropy 함수가 왜 같을까?
    위 식에서 y, L은 실제값,  H(x), S 는 예측값을 각각 뜻합니다.
    또한 y, L은 0 또는 1인 경우라고 가정하겠습니다.

    선지 a, b, c, d 각각의 가정 상황만으로도 쉽게 답을 찾으시겠지만, 한번씩 수식에 직접 값을
    대입해보시면서 교수님께서 어떤 이유로 두 함수가 같다고 하신건지 느껴보셨으면 좋겠습니다.
    ```

    ```
    다음 중 옳은 것을 고르세요.

    a. 실제값 = 0, 예측값 = 0 인 경우
       (Logistic Cost 함수의 값) = infinite
       (Cross Entropy 함수의 값) = infinite

    b. 실제값 = 0, 예측값 = 1 인 경우
       (Logistic Cost 함수의 값) = 0
       (Cross Entropy 함수의 값) = 0

    c. 실제값 = 1, 예측값 = 0 인 경우 
       (Logistic Cost 함수의 값) = infinite
       (Cross Entropy 함수의 값) = infinite

    d. 실제값 = 1, 예측값 = 1 인 경우 
       (Logistic Cost 함수의 값) = infinite
       (Cross Entropy 함수의 값) = infinite
    ```

    ### 출제 정보

    **출제자**

    출제자 1 : 김현우

    출제자2 : 원래 실습문제는 두 명이 출제합니다. 

    **검수자**

    검수자1 : 4조 전원 

    최종 검수자 : 신동준, 이명준, 이영인, 전서현

    **기타**

    출제일자 **:** 2021.05.12

- 풀이 3번

    답: **(c)**

    (a)
    C(0,0) = 0**log(0) - (1-0)**log(1-0) = 0
    D(0,0) = -0**log(0) - 1**log(1) = 0

    (b)
    C(1,0) = 0**log(1) - (1-0)**log(1-1) = infinite
    D(1,0) = -0**log(1) - 1**log(0) = infinite

    (c)
    C(0,1) = 1**log(0) - (1-1)**log(1-0) = infinite
    D(0,1) = -1**log(0) - 0**log(1) = infinite

    (d)
    C(1,1) = 1**log(1) - (1-1)**log(1-1) = 0
    D(1,1) = -1**log(1) - 0**log(0) = 0

    ### 기여자

    강찬울, 김영재, 양가영, 이연경, 이현재: 스터디 시간에 다함께 의견 나누었다.

    ### 검수자

    이연경

# 문제 4 : one-hot encoding 실습

---

- 문제 4번

    ### 문제타입

    - [ ]  이론
    - [x]  실습

    ---

    [wine.random.csv](%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/wine.random.csv)

    ### 문제내용

    ```
    두 종류의 와인을 구별하는 데이터 파일의 result 값이 실수로 (0, 1, 2, 3) 
    네 가지 종류로 바뀌었다. (기존의 result 값은 0과 1) 

    코드를 완성하여 프로그램이 예측하는 것을 보고 0과 1이 각각 2와 3 중 어떤 것과 
    같은 종류인지 확인해보자.

    if 문을 사용하여 프로그램이 어떻게 예측하는지 관찰하여 정답을 확인
    (밑의 그림을 참고)
    ```

    ![%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/Untitled%201.png](%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/Untitled%201.png)

    **Base Code**

    ```python
    # tf 호출
    import tensorflow.compat.v1 as tf
    tf.disable_v2_behavior()

    wine = # 파일불러오기 (csv파일)

    x_data = # input data
    y_data = # output data

    nb_classes = 4  

    X = tf.placeholder()
    Y = tf.placeholder()

    Y_one_hot = tf.one_hot(Y, nb_classes) 
    Y_one_hot = tf.reshape(Y_one_hot, []) # rank 한 단계 낮춰주기.

    W = tf.Variable(tf.random_normal([]), name='weight')  # 가중치 shape
    b = tf.Variable(tf.random_normal([]), name='bias')    # 편의 shape
     
    logits = tf.matmul(X, W) + b
    hypothesis = tf.nn.softmax(logits)

    # Cross entropy cost/loss
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,
                                                                     labels=tf.stop_gradient([Y_one_hot])))
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

    prediction = tf.argmax(hypothesis, 1)
    correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

    # Launch graph
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        for step in range(2001):
            _, cost_val, acc_val = sess.run([optimizer, cost, accuracy], feed_dict={X: x_data, Y: y_data})
                                            
            if step % 100 == 0:
                print("Step: {:5}\tCost: {:.3f}\tAcc: {:.2%}".format(step, cost_val, acc_val))

        # 예측
        pred = sess.run(prediction, feed_dict={X: x_data})
        cnt02=0;cnt03=0;cnt12=0;cnt13=0
        for p, y in zip(pred, y_data.flatten()):
    # flatten이 오류가 난다면 왜 오류가 났을지 알아보자! (iloc, values의 차이)
          print("[{}] Prediction: {} True Y: {}".format(p == int(y), p, int(y)))
          ## if 문을 사용하여 프로그램이 어떻게 예측하였는지 확인.
          if(p!= int(y)):
            if(p==0)&(int(y)==2):
              cnt02+=1
            elif(p==0)&(int(y)==3):
              cnt03+=1
            elif(p==1)&(int(y)==2):
              cnt12+=1
            elif(p==1)&(int(y)==3):
              cnt13+=1
        print('---------------------------------------------------------')

    print(cnt02,cnt03)
    print(cnt12,cnt13)
    ```

    ---

    ## 출제 정보

    ### 출제 의도

    **one-hot encoding을 잘 이해하였는지 확인하고 오랜만에 간단한 python 문법을 사용하여 문제를 해결해보자.**

    ### 출제 정보

    **출제자**

    출제자 1 : 정근수

    **검수자**

    검수자1 : 4조 전원

    최종 검수자 : 신동준, 이명준, 이영인, 전서현(tf 호출하는 코드 추가했습니다.)

    **기타**

    출제일자 **:** 2021.05.12

- 풀이 4번

    ```python

    # tf 호출
    import tensorflow.compat.v1 as tf
    tf.disable_v2_behavior()

    wine = pd.read_csv('wine.random.csv') # 파일불러오기 (csv파일)

    x_data = np.array(wine.iloc[:,0:12]) # input data
    y_data = np.array(wine.iloc[:,[-1]]) # output data

    nb_classes = 4 

    X = tf.placeholder(tf.float32,[None,12])
    Y = tf.placeholder(tf.int32,[None,1])

    Y_one_hot = tf.one_hot(Y, nb_classes) 
    Y_one_hot = tf.reshape(Y_one_hot, [-1,nb_classes]) # rank 한 단계 낮춰주기.

    W = tf.Variable(tf.random_normal([12,nb_classes]), name='weight')  # 가중치 shape
    b = tf.Variable(tf.random_normal([nb_classes]), name='bias')    # 편의 shape
     
    logits = tf.matmul(X, W) + b
    hypothesis = tf.nn.softmax(logits)

    # Cross entropy cost/loss
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,
                                                                     labels=tf.stop_gradient([Y_one_hot])))
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

    prediction = tf.argmax(hypothesis, 1)
    correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

    # Launch graph
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        for step in range(2001):
            _, cost_val, acc_val = sess.run([optimizer, cost, accuracy], feed_dict={X: x_data, Y: y_data})
                                            
            if step % 100 == 0:
                print("Step: {:5}\tCost: {:.3f}\tAcc: {:.2%}".format(step, cost_val, acc_val))

       # 예측
        pred = sess.run(prediction, feed_dict={X: x_data})
        cnt02=0;cnt03=0;cnt12=0;cnt13=0
        for p, y in zip(pred, y_data.flatten()):
    # flatten이 오류가 난다면 왜 오류가 났을지 알아보자! (iloc, values의 차이)
          print("[{}] Prediction: {} True Y: {}".format(p == int(y), p, int(y)))
          ## if 문을 사용하여 프로그램이 어떻게 예측하였는지 확인.
          if(p!= int(y)):
            if(p==0)&(int(y)==2):
              cnt02+=1
            elif(p==0)&(int(y)==3):
              cnt03+=1
            elif(p==1)&(int(y)==2):
              cnt12+=1
            elif(p==1)&(int(y)==3):
              cnt13+=1
        print('---------------------------------------------------------')

    print(cnt02,cnt03)
    print(cnt12,cnt13)
    ```

    <출력결과>

    ![%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/44.png](%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/44.png)

    결과값이 항상 cnt02 <= cnt03, cnt12 >= cnt13으로 나오므로

    0과 3이 같고 1과 2가 같다는 것을 알 수 있다.

    ### 기여자

    강찬울, 김영재, 양가영, 이연경, 이현재: if문을 사용하여 프로그램이 어떻게 예측하였는지 확인하라는 조건을 이해를 하지 못하였다.

    -

    김영재: 대표로 출제자님께 질문을 하여, False라고 출력된 결과들을 살펴본 후 프로그램이 어떻게 다르게 예측을 했는지 카운트해서 경향성을 찾아보면 된다는 답변을 받았다.

    -

    양가영: 가장 간단하게 cnt를 체크할 수 있는 if문을 제시하였다.

    ### 검수자

    이연경

# 문제 5 : Irises flower

---

- 문제 5번

    ## 문제타입

    - [ ]  이론
    - [x]  실습

    ---

    ## 문제내용

    [Fishers Iris Data.csv](%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/Fishers_Iris_Data.csv)

    ```
    위의 데이터는 붓꽃(Irises flower)의 데이터 입니다.
    해당 데이터에는 Setosa, Versicolor, Virginica 3종의 붓꽃 데이터가 들어 있습니다.

    붓꽃은 Petal Width, Petal Length, Sepal Width, Sepal Length이라는 데이터를 통해 분류가 가능합니다.
    softmax를 사용해서 해당 꽃이 어느 종인지 분류해보세요
    ```

    ```python
    import tensorflow.compat.v1 as tf
    import numpy as np
    import pandas as pd

    tf.disable_v2_behavior()
    tf.set_random_seed(777)  # for reproducibility

    df =  csv 파일을 읽어와주세요(불러오는 방법은 상관 없습니다) 

    '''
    문제에 제시한대로 파일로부터 데이터 셋을 저장해주세요
    '''
    x_data =  수정 
    y_data =  수정 

    '''
    nb_classes는 y_data에서 나올 수 있는 종류 개수입니다
    '''
    nb_classes =  수정 

    '''
    각각의 데이터 셋에 맞게 shape로 맞춰주세요
    '''
    X = tf.placeholder(tf.float32, shape=[ shape를 수정해주세요 ])
    Y = tf.placeholder(tf.int32, shape=[ shape를 수정해주세요 ])

    '''
    Y에 맞게 one hot encoding을 진행해주세요
    이 때 shape에 주의해주세요
    '''
    Y_one_hot =   

    '''
    데이터에 맞게 W와 b를 선언해주세요
    '''
    W = tf.Variable(tf.random_normal([ shape를 수정해주세요 ]), name='weight')
    b = tf.Variable(tf.random_normal([ shape를 수정해주세요 ]), name='bias')

    logits = tf.matmul(X, W) + b
    hypothesis =  softmax 함수를 사용해서 가설을 세워보세요 

    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,
                                                                     labels=tf.stop_gradient([Y_one_hot])))
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

    prediction = tf.argmax(hypothesis, 1)
    correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        for step in range(2001):
            _, cost_val, acc_val = sess.run([optimizer, cost, accuracy],
                                       feed_dict={X: x_data, Y: y_data})
                                            
            if step % 100 == 0:
                print(sess.run(W))

        # 아래의 출력 결과로부터 예측한 값이 맞는지 확인해보세요
        for p, y in zip(pred, y_data.flatten()):
            print("[{}] Prediction: {} True Y: {}".format(p == int(y), p, int(y)).encode())

        # 학습한 데이터를 통해 정확도를 확인해보세요
        corr, acc = sess.run([prediction, accuracy], feed_dict={X: x_data, Y: y_data})
        print("Correct :", corr)
        print("accuracy :", acc)
    ```

    대충 `0.9x`이상이 뜨면 됩니다.

    ![%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/Untitled%202.png](%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/Untitled%202.png)

    ## 출제 정보

    **출제의도**

    **출제자 - 신동준**

    출제자 1 : 분류를 하기 위한 여러 데이터를 찾아보다가 붓꽃의 데이터가 softmax에 적합한 사례인 것 같아서 데이터를 구해 출제하게 되었습니다.

    **검수자**

    검수자1 : 4조 전원

    최종 검수자 : 이명준, 이영인, 전서현

    **기타**

    출제일자 **:** 2021.05.11

- 풀이 5번

    ```python
    # 문제 5
    import tensorflow.compat.v1 as tf
    import numpy as np
    import pandas as pd

    tf.disable_v2_behavior()
    tf.set_random_seed(777)  # for reproducibility

    df =  pd.read_csv('Fishers Iris Data.csv') 

    '''
    문제에 제시한대로 파일로부터 데이터 셋을 저장해주세요
    '''
    x_data =  np.array(df.iloc[:,0:-1])
    y_data =  np.array(df.iloc[:,[-1]])
    print(x_data,y_data)

    '''
    nb_classes는 y_data에서 나올 수 있는 종류 개수입니다
    '''
    nb_classes =  3

    '''
    각각의 데이터 셋에 맞게 shape로 맞춰주세요
    '''
    X = tf.placeholder(tf.float32, shape=[None,4])
    Y = tf.placeholder(tf.int32, shape=[None,1])

    '''
    Y에 맞게 one hot encoding을 진행해주세요
    이 때 shape에 주의해주세요
    '''
    Y_one_hot = tf.one_hot(Y,nb_classes)
    Y_one_hot = tf.reshape(Y_one_hot,[-1,nb_classes])

    '''
    데이터에 맞게 W와 b를 선언해주세요
    '''
    W = tf.Variable(tf.random_normal([4,nb_classes]), name='weight')
    b = tf.Variable(tf.random_normal([nb_classes]), name='bias')

    logits = tf.matmul(X, W) + b
    hypothesis =  tf.nn.softmax(logits)

    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,
                                                                     labels=tf.stop_gradient([Y_one_hot])))
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

    prediction = tf.argmax(hypothesis, 1)
    correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        for step in range(2001):
            _, cost_val, acc_val = sess.run([optimizer, cost, accuracy],
                                       feed_dict={X: x_data, Y: y_data})
                                            
            if step % 100 == 0:
                print(sess.run(W))

        # 아래의 출력 결과로부터 예측한 값이 맞는지 확인해보세요
        for p, y in zip(pred, y_data.flatten()):
            print("[{}] Prediction: {} True Y: {}".format(p == int(y), p, int(y)).encode())

        # 학습한 데이터를 통해 정확도를 확인해보세요
        corr, acc = sess.run([prediction, accuracy], feed_dict={X: x_data, Y: y_data})
        print("Correct :", corr)
        print("accuracy :", acc)
    ```

    **<출력 결과>**

    ![%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/42.png](%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/42.png)

    ### 기여자

    강찬울, 김영재, 양가영, 이연경, 이현재: 스터디 시간에 코드를 비교해보니 모두 동일하게 나왔음을 확인할 수 있었다.

    ### 검수자

    이연경

# 보너스 문제  : Keras 다뤄보기

---

- 보너스

    ## 문제 타입

    - [ ]  이론
    - [x]  실습

    ---

    ### 문제내용

    ```
    keras함수를 이용하여 간단한 다중분류함수 코드를 만들었다. 
    빈칸을 적절히 채우고, [2,3] 를 대입했을 때 값이 어떻게 나오는지 구하시오 (0~2사이의 값)
    ```

    ---

    ```python

    import tensorflow as tf
    import numpy as np

    x_train = np.array([
    		[2, 1], 
        [3, 2],
        [3, 4],
        [5, 5],
        [7, 5],
        [2, 5],
        [8, 9],
        [9, 10],
        [6, 12],
        [9, 2],
        [6, 10],
        [2, 4]
    ])
    y_train = np.array([
    		[0 ,0, 1], 
        [0 ,0, 1], 
        [0 ,0, 1], 
        [0 ,1, 0], 
        [0 ,1, 0], 
        [0 ,0, 1], 
        [1 ,0, 0], 
        [1 ,0, 0], 
        [1 ,0, 0], 
        [0 ,1, 0], 
        [1 ,0, 0], 
        [0 ,0, 1]
    ])

    #모델 생성
    tf.model = tf.keras.Sequential()

    #층 추가
    tf.model.add(tf.keras.layers.Dense(input_dim=         , units=         , use_bias=True))
    tf.model.add(tf.keras.layers.Activation('         '))

    #다중분류손실함수를 이용하여 컴파일
    tf.model.compile(loss='       ', optimizer=tf.keras.optimizers.SGD(lr=1e-2), metrics=['accuracy'])

    #학습하기 (epochs 값을 변형하며 구해보세요)
    tf.model.fit(x_train, y_train, epochs=       )

    #예측하기
    result = tf.model.predict([       ])

    print(result,tf.keras.backend.eval(tf.argmax(result, axis=1)))

    ```

    ### 출제 정보

    **출제의도** 

    앞으로 사용하게 될 tf2버전에서 사용하는 keras 함수에 대해 미리 공부해보고, 기존  tf1 에서 사용하던 코드와 차이점이 무엇인지 확인해보자. 

    **출제자 - 이명준**

    **검수자**

    검수자1 : 4조 전원

    최종 검수자 : 신동준, 이영인, 전서현

    **기타**

    출제일자 **:** 2021.05.11

- 보너스 풀이

    ```python
    # 보너스 문제
    import tensorflow as tf
    import numpy as np

    x_train = np.array([
    		[2, 1], 
        [3, 2],
        [3, 4],
        [5, 5],
        [7, 5],
        [2, 5],
        [8, 9],
        [9, 10],
        [6, 12],
        [9, 2],
        [6, 10],
        [2, 4]
    ])
    y_train = np.array([
    		[0 ,0, 1], 
        [0 ,0, 1], 
        [0 ,0, 1], 
        [0 ,1, 0], 
        [0 ,1, 0], 
        [0 ,0, 1], 
        [1 ,0, 0], 
        [1 ,0, 0], 
        [1 ,0, 0], 
        [0 ,1, 0], 
        [1 ,0, 0], 
        [0 ,0, 1]
    ])

    #모델 생성
    tf.model = tf.keras.Sequential()

    #층 추가
    tf.model.add(tf.keras.layers.Dense(input_dim=2, units=3, use_bias=True))
    tf.model.add(tf.keras.layers.Activation('softmax'))

    #다중분류손실함수를 이용하여 컴파일
    tf.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(lr=1e-2), metrics=['accuracy'])

    #학습하기 (epochs 값을 변형하며 구해보세요)
    tf.model.fit(x_train, y_train, epochs=2000)

    #예측하기
    result = tf.model.predict(np.array([[2,3]]))

    print(result,tf.keras.backend.eval(tf.argmax(result, axis=1)))
    ```

    **<출력 결과>**

    ![%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/43.png](%E1%84%82%E1%85%A6%E1%84%87%E1%85%A5%E1%86%AB%E1%84%8D%E1%85%A2%20%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20a7f03c3e07ac4e9f83df031f4d0d6df7/43.png)

    ### 기여자

    강찬울, 김영재, 양가영, 이연경, 이현재: 스터디 시간에 코드를 비교해보니 모두 동일하게 나왔음을 확인할 수 있었다.

    ### 검수자

    이연경